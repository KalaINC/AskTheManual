# AskTheManual ‚Äì A Multimodal RAG-PoC

**AskTheManual** ist ein Proof of Concept (PoC) f√ºr Multimodale Retrieval-Augmented Generation (RAG), das entwickelt wurde, um Handb√ºcher f√ºr Ihre Kunden zu einem Chatbot umzufunktionieren. Im Gegensatz zu standardm√§√üigen RAG-Systemen, die nur Text verarbeiten, extrahiert diese Pipeline Bilder aus PDFs, analysiert sie mithilfe von Vision AI und integriert diesen visuellen Kontext in eine durchsuchbare Wissensdatenbank.

##  Was es ist
Dieses Projekt verwandelt statische PDF-Handb√ºcher in einen interaktiven, bildbewussten Chatbot. Es folgt einer mehrstufigen Pipeline:
1.  **Extraktion:** Verwendet `Docling`, um PDFs in Markdown zu konvertieren, wobei Tabellenstrukturen erhalten bleiben und Bilder extrahiert werden.
2.  **Menschliche √úberpr√ºfung (Human-in-the-Loop):** Erm√∂glicht Benutzern, "Junk"-Bilder (Symbole, dekorative Elemente) vor der Verarbeitung herauszufiltern.
3.  **Visuelle Anreicherung:** Verwendet OpenAIs Vision-Modelle, um Screenshots zu beschreiben (z.B. "Fenster: Einstellungen, Wert: Server-IP: 127.0.0.1") und so Pixel in durchsuchbaren Text umzuwandeln.
4.  **Vektorindizierung:** Zerlegt das angereicherte Markdown in Chunks und speichert es in einer `FAISS`-Vektordatenbank unter Verwendung von `MiniLM`-Embeddings.
5.  **Lokaler Chat:** Ein `Streamlit`-Dashboard, das die Datenbank abfragt und Antworten mithilfe einer lokalen `Ollama`-Instanz generiert.

##  Vorteile

###  Lokale Kontrolle & Datenschutz
Durch die lokale Nutzung von **Ollama** und **FAISS** bleibt das Kern-"Gehirn" Ihres Chatbots auf Ihrer oder der Infrastruktur des Kunden. Ihre propriet√§ren Handb√ºcher werden nicht zur Generierung der endg√ºltigen Antwort an ein Drittanbieter-LLM gesendet, wodurch die Datenhoheit gew√§hrleistet ist.

###  Keine "Black Box"
Im Gegensatz zu propriet√§ren "Black Box"-L√∂sungen bietet AskTheManual dem Dokumentenbesitzer vollst√§ndige Transparenz und Kontrolle √ºber die gesamte Pipeline:
*   **Extraktionskontrolle:** Sehen Sie genau, welcher Text und welche Tabellen extrahiert werden, bevor sie in die Datenbank gelangen.
*   **Bildkuratierung:** Sie entscheiden, welche Bilder relevant sind und welche "M√ºll" sind.
*   **Flexible Anreicherung:** Sie k√∂nnen Cloud Vision AI f√ºr Beschreibungen verwenden, Ihre eigenen manuellen Erkl√§rungen im Markdown verfassen oder das LLM rohe Bilder mit dem umgebenden Text assoziieren lassen.
*   **Modelltransparenz:** Sie w√§hlen aus, welche lokalen LLMs und Embedding-Modelle verwendet werden, um sicherzustellen, dass Sie genau wissen, wie Ihre Daten verarbeitet werden.

###  Multimodales Verst√§ndnis
Die meisten RAG-Systeme sind "blind" f√ºr Bilder. AskTheManual behandelt Screenshots als erstklassige Elemente. Durch die Indizierung von Beschreibungen dessen, was *in* einem Screenshot enthalten ist (Felder, Kontrollk√§stchen, Pfade), kann die KI Fragen wie "Wie sollte die Standard-Server-IP im Einstellungsfenster aussehen?" beantworten, selbst wenn diese Informationen nur visuell vorhanden sind.

###  Human-in-the-Loop
Der Extraktionsprozess beinhaltet einen √úberpr√ºfungsschritt. Dies stellt sicher, dass nur relevante technische Diagramme und Screenshots in den Vektorspeicher gelangen, wodurch der Index sauber und das Kontextfenster der KI fokussiert bleibt.

![Workflow](./workflow_DE.svg)

##  Abh√§ngigkeiten

Das Projekt basiert auf den folgenden Kernbibliotheken:
- **UI:** `streamlit`
- **PDF-Verarbeitung:** `docling`
- **Vektorspeicher:** `faiss-cpu`, `langchain-community`
- **Embeddings:** `langchain-huggingface`, `sentence-transformers`
- **LLM-Integration:** `requests` (f√ºr Ollama API), `openai` (f√ºr Vision-Anreicherung)

## Installation & Einrichtung

### 1. Python-Anforderungen installieren
Stellen Sie sicher, dass Python 3.10+ installiert ist, und f√ºhren Sie dann Folgendes aus:
```bash
pip install streamlit docling langchain-huggingface langchain-community faiss-cpu sentence-transformers requests
```

### 2. Ollama einrichten (Lokales LLM)
- Laden Sie **Ollama** von ollama.com herunter und installieren Sie es.
- Laden Sie das ben√∂tigte Modell herunter:
  ```bash
  ollama pull qwen2.5:7b
  ```
- Stellen Sie sicher, dass der Ollama-Server l√§uft (normalerweise auf Port 11434).

### 3. DocLing einrichten
DocLing wird f√ºr hochpr√§zises PDF-Parsing verwendet. Es wird √ºber pip installiert (in Schritt 1 enthalten). Beim ersten Start l√§dt es m√∂glicherweise notwendige KI-Modelle f√ºr die Layout-Analyse herunter.

### 4. Embeddings (MiniLM)
Das Projekt verwendet `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`. Sie m√ºssen dies nicht manuell herunterladen; `langchain-huggingface` wird es beim ersten Indexierungslauf automatisch abrufen.

### 5. OpenAI API-Schl√ºssel (f√ºr Vision)
Um das Skript `image_to_information.py` zu verwenden, ben√∂tigen Sie einen OpenAI API-Schl√ºssel.
- Exportieren Sie ihn als Umgebungsvariable oder bearbeiten Sie die Variable `OPENAI_API_KEY` im Skript.  

Sie k√∂nnen nat√ºrlich jeden bevorzugten KI-Anbieter Ihrer Wahl verwenden. Stellen Sie einfach sicher, dass er Bilder als Eingabe verarbeiten kann.  

## üìÇ Projekt-Workflow

### 1. Erfassung & √úberpr√ºfung (Erforderlich)
F√ºhren Sie `unified_extraction_review.py` aus, um Ihr PDF zu verarbeiten. Dieser Schritt beinhaltet eine **Human-in-the-Loop**-√úberpr√ºfung, bei der Sie manuell ausw√§hlen, welche Bilder beibehalten und welche verworfen werden sollen.

### 2. Anreicherung (Optional)
Erweitern Sie Ihre Dokumentation, indem Sie KI-generierte Beschreibungen zu extrahierten Bildern hinzuf√ºgen.
*   **Vorschau & Kostenoptimierung:** Bevor Sie die vollst√§ndige Anreicherung durchf√ºhren, verwenden Sie `image_to_information_testing.py`. Dieses Skript generiert eine Datei `openai_prompts_preview.json`, die es Ihnen erm√∂glicht, den genauen Textkontext und die API-Nutzlast zu √ºberpr√ºfen, die an die Vision AI gesendet w√ºrden. Dies ist entscheidend, um zu √ºberpr√ºfen, ob der "debug_context_used" korrekt ist, bevor Ihnen Token-Kosten entstehen.
*   **Ausf√ºhrung:** F√ºhren Sie `image_to_information.py` aus, um die eigentliche Analyse durchzuf√ºhren und Ihr Markdown mit `[AI-ANALYSIS]`-Tags zu aktualisieren.

> **Hinweis:** Wenn Sie diesen Schritt √ºberspringen, stellen Sie sicher, dass Sie den System-Prompt in `chatbot_dashboard.py` anpassen. Ohne Anreicherung sollte das LLM angewiesen werden, Bilder basierend auf ihrer N√§he zu relevantem Text zu referenzieren, anstatt sich auf beschreibende KI-Analyse-Tags zu verlassen.

### 3. Index & Chat
*   **Index:** F√ºhren Sie `vector_transformer.py` aus, um die FAISS-Vektordatenbank zu erstellen oder zu aktualisieren.
*   **Chat:** Starten Sie das interaktive Dashboard:
    ```bash
    streamlit run chatbot_dashboard.py
    ```

---

### ‚ö†Ô∏è Haftungsausschluss
Dieses PoC ist derzeit f√ºr Demonstrations- und interne Testzwecke vorgesehen. Das bereitgestellte Dashboard ist ein visueller Prototyp, um die Technologie zu pr√§sentieren. F√ºr den Produktionseinsatz sollten Sie eine benutzerdefinierte Chat-Oberfl√§che entwickeln, die auf Ihre spezifische Softwareumgebung zugeschnitten ist, und evaluieren, ob die KI-Modelle auf lokaler Kundenhardware oder Ihren eigenen oder zentralen sicheren Servern von Drittanbietern gehostet werden sollen.

---
*Entwickelt als PoC f√ºr Documentation Intelligence.*